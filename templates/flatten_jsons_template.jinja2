#!{{ path_to_python }}

import multiprocessing
import glob
import os
import pandas as pd
from subprocess import Popen, PIPE
import argparse
from multiprocessing import Process, Manager
import numpy as np
from pandas import read_csv


def flatten_json_2(input_file):
    output_file = input_file.replace(".json", ".csv")
    output_file = output_file.replace(".djson", ".csv")

    args = [
        "json2csv",
        "--flatten-objects",
        "--flatten-arrays",
        "--flatten-separator",
        "/",
        "-i",
        input_file,
        "-o",
        output_file,
    ]

    p = Popen(args, stdout=PIPE, stderr=PIPE)
    stdout, stderr = p.communicate()
    if p.returncode == 0:
        return output_file
    else:
        print(stderr)
    return None


def flatten_files(files, dataframe):
    for file in files:
        csv_file = flatten_json_2(file)
        if csv_file is not None:
            temp = read_csv(csv_file)
            cols = []
            for col in temp.columns:
                cols.append(col.replace(".", ""))
            temp.columns = cols
            dataframe.append(temp)


def main(raw_args=None):
    parser = argparse.ArgumentParser()
    parser.add_argument("--dummy_json", required=True, help="Path to dummy JSON")
    parser.add_argument("--path_to_jsons", required=True, help="Path to JSON files")
    parser.add_argument("--csv_file", required=True, help="CSV file to generate")
    args = parser.parse_args(raw_args)

    if not os.path.exists(args.dummy_json):
        print("Dummy file does not exist")
        exit(1)
    if not os.path.exists(args.path_to_jsons):
        print("JSON directory does not exist")
        exit(1)

    df = Manager().list()

    dummy_csv = flatten_json_2(args.dummy_json)
    if dummy_csv is None:
        print("Error while flattening the dummy JSON")
        exit(1)
    temp = read_csv(dummy_csv)
    cols = []
    for col in temp.columns:
        cols.append(col.replace(".", ""))
    temp.columns = cols
    df.append(temp)

    paths = ["*.json"]
    out_path2 = os.path.join(args.path_to_jsons, *paths)
    files = glob.glob(out_path2)
    if len(files) == 0:
        print("There are no JSON files to process")
        exit(1)

    # print("Processing with {} threads".format(multiprocessing.cpu_count()))
    arrays = np.array_split(np.array(files), multiprocessing.cpu_count()-2)

    threads = list()
    for ii in range(len(arrays)):
        keywords = {"files": arrays[ii], "dataframe": df}
        process = Process(target=flatten_files, kwargs=keywords)
        process.start()
        threads.append(process)

    for process in threads:
        process.join()
    join = pd.concat(df, sort=False)
    join = join.iloc[1:]
    join.to_csv(args.csv_file, index=False, encoding="utf-8")


if __name__ == "__main__":
    main()
